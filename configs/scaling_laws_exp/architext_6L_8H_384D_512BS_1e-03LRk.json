{
    "layers": 6,
    "d_model": 384,
    "n_heads": 8,
    "n_vocab": 50400,
    "norm": "layernorm",
    "pe": "rotary",
    "pe_rotary_dims": 64,
    "seq": 2048,
    "cores_per_replica": 8,
    "per_replica_batch": 2,
    "gradient_accumulation_steps": 256,
    "warmup_steps": 1260,
    "anneal_steps": 11340,
    "lr": 0.001277097798956528,
    "end_lr": 0.0001277097798956528,
    "weight_decay": 0.1,
    "total_steps": 12600,
    "tpu_size": 8,
    "bucket": "all-models",
    "model_dir": "models/EleutherAI/GPT-J/scaling_laws/big_experiment/6L_8H_384D_512BS_1e-03LR",
    "train_set": "architext_v2.train.index",
    "val_set": {
        "architext": "architext_v2.valid.index"
    },
    "eval_harness_tasks": [],
    "val_batches": 100,
    "val_every": 100,
    "ckpt_every": 1000,
    "keep_every": 10000,
    "name": "6L_8H_384D_512BS_1e-03LR",
    "wandb_project": "GPT-J-Architext",
    "comment": ""
}